{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to dl_312_venv (Python 3.12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c49aa54-41b3-4452-ba1e-d401880d33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Within epoch iterations:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Epoch loop:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\nlp_with_dl_from_scratch\\word2vec\\train_simple_cbow_pt.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m cbow_model \u001b[39m=\u001b[39m SimpleCBOWPT(vocab_size, hidden_size)\n\u001b[0;32m     44\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(cbow_model\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m losses \u001b[39m=\u001b[39m training_loop(max_epoch, optimizer, cbow_model, loss_fn, cbow_dataloader, cbow_dataloader, pathlib\u001b[39m.\u001b[39;49mPath\u001b[39m.\u001b[39;49mcwd()\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     46\u001b[0m plot_losses(losses)\n",
      "File \u001b[1;32mc:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\nlp_with_dl_from_scratch\\word2vec\\..\\word2vec\\simple_cbow_pytorch.py:51\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, models_folder)\u001b[0m\n\u001b[0;32m     48\u001b[0m targets \u001b[39m=\u001b[39m contexts\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     49\u001b[0m scores \u001b[39m=\u001b[39m model(contexts)\n\u001b[1;32m---> 51\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(scores, targets)\n\u001b[0;32m     52\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\nlp_with_dl_from_scratch\\word2vec\\..\\word2vec\\simple_cbow_pytorch.py:11\u001b[0m, in \u001b[0;36mloss_fn\u001b[1;34m(scores, targets)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(scores, targets):\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy(scores, targets)\n",
      "File \u001b[1;32mc:\\Users\\amrul\\pyvirtualenvs\\dl_312_venv\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from ngram_model.utils import build_corpus, get_id_to_word, get_word_to_id_from_vocab, build_vocabulary\n",
    "from word2vec.simple_cbow import SimpleCBOW\n",
    "from common.trainers import Trainer\n",
    "from common.optimizers import Adam\n",
    "from common.utils import create_contexts_and_targets, convert_one_hot\n",
    "from ngram_model.utils import tokenize, build_vocabulary, get_word_to_id_from_vocab, get_id_to_word, build_corpus\n",
    "from word2vec.simple_cbow_pytorch import compute_loss, training_loop,SimpleCBOWPT, CBOWDataset,plot_losses,loss_fn\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 100\n",
    "\n",
    "text = \"Prince loved princess from the bottom of his heart\"\n",
    "\n",
    "words = tokenize(text)\n",
    "vocab = build_vocabulary(words)\n",
    "word_to_id = get_word_to_id_from_vocab(vocab)\n",
    "id_to_word = get_id_to_word(word_to_id)\n",
    "corpus = build_corpus(words, word_to_id)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, targets = create_contexts_and_targets(corpus, window_size)\n",
    "contexts = convert_one_hot(np.array(contexts), vocab_size)\n",
    "targets = convert_one_hot(np.array(targets), vocab_size)\n",
    "\n",
    "contexts = torch.tensor(contexts, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "cbow_dataset = CBOWDataset(contexts, targets)\n",
    "cbow_dataloader = DataLoader(cbow_dataset, batch_size, shuffle=True)\n",
    "\n",
    "cbow_model = SimpleCBOWPT(vocab_size, hidden_size)\n",
    "optimizer = optim.Adam(cbow_model.parameters(),lr=0.001)\n",
    "losses = training_loop(max_epoch, optimizer, cbow_model, loss_fn, cbow_dataloader, cbow_dataloader, pathlib.Path.cwd()/\"checkpoint\")\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape : torch.Size([3, 9])\n",
      "batch_y shape : torch.Size([3, 9])\n",
      "scores shape : torch.Size([3, 9])\n",
      "batch_y shape : torch.Size([3, 9])\n",
      "scores shape : torch.Size([1, 9])\n",
      "batch_y shape : torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "for idx, (batch_x, batch_y) in enumerate(cbow_dataloader):\n",
    "    scores = cbow_model(batch_x)\n",
    "    print(f\"scores shape : {scores.size()}\")\n",
    "    print(f\"batch_y shape : {batch_y.size()}\")\n",
    "    loss = loss_fn(scores, batch_y)\n",
    "    if idx>3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_312_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
